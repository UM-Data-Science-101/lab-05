{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: American Community Survey\n",
        "jupyter:\n",
        "  jupytext:\n",
        "    cell_metadata_filter: '-all'\n",
        "    main_language: python\n",
        "    notebook_metadata_filter: '-all'\n",
        "  kernelspec:\n",
        "    display_name: Python 3 (ipykernel)\n",
        "    language: python\n",
        "    name: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The American Community Survey (ACS) is a large survey of households\n",
        "and individuals in the United States, carried out by the US\n",
        "government on a continuous basis (around 3.5 million people are\n",
        "contacted per year).  It is arguably the most authoritative source\n",
        "of information about the demographic composition of the US\n",
        "population, and is used for many purposes in academic research,\n",
        "government, public policy, and in private industry.\n",
        "\n",
        "Some of the questions in the ACS are about sensitive topics, and therefore are only released in aggregate form. The \"public use microsample\" (PUMS) is a set of individual ACS responses that only includes information that has been deemed safe for public release at the individual level. Here we will work with a subset of the ACS/PUMS data.\n",
        "\n",
        "You will need to refer to the documentation to know what the ACS variable names mean: **[ACS PUMS Codebooks](https://www.census.gov/programs-surveys/acs/microdata/documentation/2018.html)** Scroll down for data dictionary 2018 \"1-year\" ACS/PUMS, available in several formats.\n",
        "\n",
        "For this course, we are providing a simplified version of the ACS/PUMS data from 2018. It contains a random subset of the cases and a selected subset of variables.\n",
        "\n",
        "Note that many PUMS variables are described as being \"household\" or \"individual\" variables. These refer to characteristics of households (one or more people living at the same address) or to characteristics of individual people, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import scipy.stats as sps\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we load the data from the filesystem into a dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(\"./pums_short.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing values\n",
        "\n",
        "Most datasets in the real world have missing values.  There are many\n",
        "reasons that a value may be missing.  For example, in some cases it\n",
        "makes no sense to calculate a number (e.g. income for a young\n",
        "child), or a person may refuse to answer a question in a survey, or\n",
        "a value may have inadvertently not been collected.\n",
        "\n",
        "In Pandas, missing values are represented using the symbol `NaN` (we can refer to it in programs using `np.nan` if we need to),\n",
        "which means \"not a number\".  The method `isnull` tells us which\n",
        "values in a Pandas data structure (a dataframe or series) are missing (\"null\" is a computer science term of missing or \"not yet recorded\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "s_with_nan = pd.Series([1, np.nan, 2, 10])\n",
        "s_with_nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "s_with_nan.isnull()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that if we take the mean of boolean values we get a proportion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "s_with_nan.isnull().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to know the proportion of observations for each variable\n",
        "that are missing, we can use `isnull` and `mean` on an entire table. Do so for the `df` table.\n",
        "\n",
        "\n",
        "<details>\n",
        "\n",
        "```\n",
        "df.isnull().mean()\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "The above results shows us that a few variables, e.g. RNTP, have\n",
        "many missing values, while others, e.g. DIVISION, have no missing\n",
        "values.  The RNTP variable contains information about the amount of\n",
        "rent someone pays, so is always missing if a person does not rent\n",
        "the place where they live.  DIVISION indicates where in the US the\n",
        "person lives.  DIVISION can never be missing because the census\n",
        "bureau always has this information about each respondent.\n",
        "\n",
        "If you want to drop all cases (rows) of a variable that are missing,\n",
        "you can use the `dropna` method.  For example, suppose we want the\n",
        "non-missing values for the amount of money that households pay for\n",
        "rent (RNTP).  As noted above, these values are missing for\n",
        "households that do not rent their place of residence.  We can use\n",
        "the following code to obtain the non-missing values and store them\n",
        "in a series called x:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = df[\"RNTP\"].dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If is often useful to \"chain\" `dropna` with other methods or\n",
        "attributes of a series.  For example, if we want to know how many\n",
        "people have a non-missing rent value (i.e. are renters), we can use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The treatment of missing values in a statistical analysis is a\n",
        "complex topic.  Here we are simply demonstrating how to drop the\n",
        "missing values from the dataset.  Sometimes this is the correct\n",
        "thing to do, but often it is not.\n",
        "\n",
        "## Summary statistics of income\n",
        "\n",
        "We will turn now to the variable \"HINCP\" which is the household\n",
        "income -- the combined income of everyone living in one household.\n",
        "Note that income is not the same thing as wealth.  Many people have\n",
        "far more wealth than income.\n",
        "\n",
        "Overall, the United States has the following income characteristics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"HINCP\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us observe two things:\n",
        "\n",
        "1. The summary statistics above are computed using the non-missing\n",
        "values of HINCP.  Most Pandas dataframe methods, like `describe`,\n",
        "automatically drop missing values, but functions and methods from\n",
        "other Python packages generally do not, so in some settings you will\n",
        "need to drop the missing values explicitly using `dropna`.\n",
        "\n",
        "2. The data are printed out in \"scientific notation\" such that `3e+02` means $3 \\cdot 10^2 = 300$. To make this a little easier to read, let's rewrite this as \"income in thousands of dollars\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"HINCP_1000\"] = df[\"HINCP\"] / 1000\n",
        "df[\"HINCP_1000\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use a `histplot` or `kdeplot` to show the distribution of `\"HINCP_1000\"`. Observe the the connection with location (`mean` and `50%`, the median) and variation (`std` and the difference of `25%` and `75%`).\n",
        "\n",
        "\n",
        "You will notice that a very small fraction of the household income\n",
        "values are negative, as shown below.  This is not an error.  Using\n",
        "standard definitions for income, it is possible for a household (or\n",
        "an individual) to have negative income.  Compute the proportion of\n",
        "households with negative income. (Hint: remember that a proportion is the mean of a boolean series).\n",
        "\n",
        "\n",
        "<details>\n",
        "    \n",
        "```    \n",
        "(df[\"HINCP_1000\"] < 0).mean()\n",
        "```\n",
        "                        \n",
        "</details>\n",
        "\n",
        "## Spread and Skew\n",
        "\n",
        "We already went through a couple measures of spread, even if we did not realize! Let's check again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"HINCP_1000\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apart from the mean, what gives us an idea of how spread out or skewed our data actually is?  In other words, how can other quantities help in giving us an idea of the behavior of the data like spread and skew?\n",
        "\n",
        "### Spread\n",
        "\n",
        "We will be taking a look at different measures of spread in order to quantify the variation within our data. In particular, we will be looking at three measures of spread:\n",
        "\n",
        "- **Range**: This is the simplest measure of spread, calculated as the difference between the maximum and minimum values.\n",
        "- **Standard deviation**: Measures the variation of values of a variable about their mean.\n",
        "- **Quantiles**: Divide data into regions of equal probability, such as quartiles or percentiles, which can give you insights on the distribution of the data. From these measures of spread, quantiles are the most robust to outliers.\n",
        "\n",
        "#### Range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "income_min = df[\"HINCP\"].min()\n",
        "income_max = df[\"HINCP\"].max()\n",
        "income_range = income_max - income_min\n",
        "print(f\"Minimum Income: {income_min}, Maximum Income: {income_max}, Range: {income_range}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The range by itself does not tell the full story, as this is a measure of spread that is specially sensitive to outliers. For instance, a single very high income could make the range extremely large, while the rest of the values are closer on average.\n",
        "\n",
        "#### Standard Deviation\n",
        "\n",
        "The standard deviation (and the variance!) measure spread by calculating the average distance of the data points from the mean. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "income_std = df[\"HINCP\"].std()\n",
        "print(f\"Standard Deviation of Household Income: {income_std}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The standard deviation, conveniently, retains the same units as the data, making it intuitive, unlike the **variance**, which is the square of the standard deviation. Although it is useful in theoretical contexts, the variance is harder to interpret in practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "income_var = df[\"HINCP\"].var()\n",
        "print(f\"Variance of Household Income: {income_var}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "High values of the standard deviation means that the data points are widely spread out, whereas low values indicate that the data is clustered together.\n",
        "\n",
        "Let's see how income data spreads one standard deviation around the mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "income_mean = df[\"HINCP\"].mean()\n",
        "income_std = df[\"HINCP\"].std()\n",
        "\n",
        "# Let's create the histogram plot with the KDE\n",
        "ax = sb.histplot(df[\"HINCP\"], kde=True, color='blue')\n",
        "\n",
        "# This code shades the area within one standard deviation around the mean\n",
        "xmin, xmax = income_mean - income_std, income_mean + income_std\n",
        "ax.axvline(income_mean, color='red', linestyle='--', label='Mean')\n",
        "ax.fill_betweenx([0, ax.lines[0].get_data()[1].max()], xmin, xmax, color='yellow', alpha=0.3, label='Within 1 Std Dev')\n",
        "\n",
        "# Always add labels and legend!\n",
        "ax.set_title('Distribution of Household Income (HINCP) with One Std Dev Shaded')\n",
        "ax.set_xlabel('Household Income')\n",
        "ax.set_ylabel('Density')\n",
        "ax.legend()\n",
        "\n",
        "sb.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quantiles\n",
        "\n",
        "Quantiles divide observations into intervals of equal probability, and they are more robust against outliers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting the distribution of income with quantiles\n",
        "ax = sb.histplot(df[\"HINCP\"], kde=True, color='blue')\n",
        "\n",
        "# Add vertical lines for Q1, median (Q2), and Q3\n",
        "Q1 = df[\"HINCP\"].quantile(0.25)\n",
        "median = df[\"HINCP\"].quantile(0.50)\n",
        "Q3 = df[\"HINCP\"].quantile(0.75)\n",
        "\n",
        "ax.axvline(Q1, color='black', linestyle='--', label='Q1 (25th Percentile or 1st Quartile)')\n",
        "ax.axvline(median, color='red', linestyle='--', label='Median (50th Percentile or 2nd Quartile)')\n",
        "ax.axvline(Q3, color='black', linestyle='--', label='Q3 (75th Percentile or 3rd Quartile)')\n",
        "\n",
        "ax.set_title('Household Income Distribution with Quartiles')\n",
        "ax.set_xlabel('Household Income')\n",
        "ax.set_ylabel('Density')\n",
        "ax.legend()\n",
        "\n",
        "sb.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the corresponding boxplot fror Household Income."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating the boxplot\n",
        "ax = sb.boxplot(x=df[\"HINCP\"], color='lightblue')\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_title('Boxplot of Household Income (HINCP)')\n",
        "ax.set_xlabel('Household Income')\n",
        "\n",
        "sb.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thought problem:** What is the correspondence between these two plots?\n",
        "\n",
        "\n",
        "**Lab problem:** Compute the $16$th and the $84$th percentiles and\n",
        "\n",
        "\n",
        "<details>\n",
        "    \n",
        "**Answer**:\n",
        "\n",
        "This is how we find quantiles using pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "P16 = df[\"HINCP\"].quantile(0.16)\n",
        "P84 = df[\"HINCP\"].quantile(0.84)\n",
        "\n",
        "print(f\"16th Percentile: {P16}\")\n",
        "print(f\"84th Percentile: {P84}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can overlay our previous histogram showing one standard deviation about the mean and the 84th and 16th percentile. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = sb.histplot(df[\"HINCP\"], kde=True, color='blue')\n",
        "\n",
        "ax.axvline(P16, color='green', linestyle='--', label='16th Percentile (P16)')\n",
        "ax.axvline(P84, color='green', linestyle='--', label='84th Percentile (P84)')\n",
        "\n",
        "xmin, xmax = income_mean - income_std, income_mean + income_std\n",
        "ax.axvline(income_mean, color='red', linestyle='--', label='Mean')\n",
        "ax.fill_betweenx([0, ax.lines[0].get_data()[1].max()], xmin, xmax, color='yellow', alpha=0.3, label='Within 1 Std Dev')\n",
        "\n",
        "\n",
        "\n",
        "ax.set_title('Household Income Distribution with 68% Range Shaded')\n",
        "ax.set_xlabel('Household Income')\n",
        "ax.set_ylabel('Density')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</details>\n",
        "\n",
        "### Skewnes\n",
        "\n",
        "Skew or skewness measure the asymmetry of the distribution, and tells us whether the data has longer tails on either side of the distribution. This is important, since many real-world variables are often skewed, and identifying skewness can guide us in our analysis, e.g. selection of methods, applicable transformations, deviation from assumptions, etc...\n",
        "\n",
        "\n",
        "**Types of Skewness**\n",
        "\n",
        "- **Symmetrical (No Skew)**: The data is evenly distributed around its mean or median. \n",
        "- **Right Skew**: The tail on the right side is longer than that on the left side. In this case, the mean is greater than the median.\n",
        "*Example*: Income tends to have a right skew. Why?\n",
        "<details>\n",
        "Income is right-skewed because most people earn around the average income, but few people earn significantly more.\n",
        "</details>\n",
        "- **Left Skew**: The tail on the left is longer thant that on the right and the mean is less than the median.\n",
        "*Example:* Death of natural causes tends to have a left skewed distribution. Again, why?\n",
        "<details>\n",
        "Most people die at an older age, but there are a few cases of death at younger ages, pulling the mean age of death down. \n",
        "</details>\n",
        "\n",
        "Skewness helps us understand the shape of our data and can guide us in our choice of methods. \n",
        "\n",
        "As we have come to expect, pandas already has functionality to compute skewness, but before that, do we expect it to be $0$, positive or a negative value? Let's take a look first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting the KDE plot to visualize skewness\n",
        "ax = sb.kdeplot(df[\"HINCP\"], fill=True, color='blue')\n",
        "ax.set_title('KDE Plot of Household Income (HINCP)')\n",
        "ax.set_xlabel('Household Income')\n",
        "ax.set_ylabel('Density')\n",
        "\n",
        "# Adding a vertical line for the mean\n",
        "income_mean = df[\"HINCP\"].mean()\n",
        "income_median = df[\"HINCP\"].median()\n",
        "ax.axvline(income_mean, color='red', linestyle='--', label='Mean')\n",
        "ax.axvline(income_median, color='green', linestyle='--', label='Median')\n",
        "\n",
        "# Adding a legend\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's compute the value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "income_skew = df[\"HINCP\"].skew()\n",
        "print(f\"Skewness of Household Income: {income_skew}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just as we predicted.\n",
        "\n",
        "## Stratifying to explain variation\n",
        "\n",
        "When considering a population such as the United States, which is\n",
        "extremely **heterogeneous** (contains high amounts of variation), it is usually much more informative to\n",
        "analyze the data after stratifying the population according to the\n",
        "values of factors that explain some of the heterogeneity.  The ACS\n",
        "includes a variable called \"FES\" which partitions the population\n",
        "into 8 subgroups based on household structure.  See the data\n",
        "dictionary for the precise definitions of the groups (note that \"LF\"\n",
        "in the documentation stands for \"labor force\").\n",
        "\n",
        "Create a box plot that shows the \"FES\" on the x-axis and \"HINCP_1000\" on the y-axis (remember that `showfliers = True` can be a useful option).\n",
        "\n",
        "\n",
        "<details>\n",
        "\n",
        "```\n",
        "sb.boxplot(data = df, x = \"FES\", y = \"HINCP_1000\", showfliers = False)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "We start to see some patterns in the data, but let us make them more precise. We can look at the set of standard summary statistics captured\n",
        "by the `describe` method, but now restricting the calculations to\n",
        "the values within each stratum (group defined by FES value).\n",
        "\n",
        "Use the `groupby` method to group the table on \"FES\". Use the `describe()` method on the `\"HINCP_1000\"` column.\n",
        "\n",
        "\n",
        "<details>\n",
        "\n",
        "```\n",
        "df.groupby(\"FES\")[\"HINCP_1000\"].describe()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "From the table above, we see that the wealthiest households, based\n",
        "on the median income, are those including a married couple, with\n",
        "both members of the couple working (group 1).  The least wealthy\n",
        "households are those including a single female who is not working\n",
        "(group 8).\n",
        "\n",
        "It is often convenient to re-order the rows of the output in order\n",
        "to sort one of the values.  Repeat the previous use of `describe` but add the `.sort_values(by = \"50%\")` to sort the data on the median column.\n",
        "\n",
        "\n",
        "<details>\n",
        "\n",
        "```\n",
        "df.groupby(\"FES\")[\"HINCP_1000\"].describe().sort_values(by = \"50%\")\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "## Multiple explanatory factors\n",
        "\n",
        "In many cases we want to stratify on two or more factors that may\n",
        "\"explain\" the variation in a value of interest.  Above we considered\n",
        "household structure as one explanatory factor.  Now we will consider\n",
        "the geographic region where the respondent lives as well.  The\n",
        "Census Bureau has several ways of partitioning by geography, the\n",
        "variable `REGION` here uses four levels (see the data dictionary for\n",
        "what they correspond to).  We will conduct a \"two-way\"\n",
        "stratification, and determine the median income for people living in\n",
        "each combination of a FES level and a REGION level.\n",
        "\n",
        "First, we create the grouping and show off the sizes of the group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grp = df.groupby([\"REGION\", \"FES\"])\n",
        "grp.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we want to compute the median within each group. Run the `median` method on the \"HINCP_1000\" column of the  `grp` object. Save the result to a variable, but then display it as well.\n",
        "\n",
        "\n",
        "<details>\n",
        "    \n",
        "```\n",
        "grp_median = grp[\"HINCP_1000\"].median()\n",
        "grp_median\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "The result of a `groupby` and computation such as `mean` or `median` expression, such as above, is\n",
        "a new dataframe in which the distinct combinations of the grouping\n",
        "variables (here, \"FES\" and \"REGION\") form the dataframe's _index_.\n",
        "\n",
        "We can make the index into a set of columns using `reset_index`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grp_sizes = grp.size().reset_index()\n",
        "grp_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This makes it possible to use Seaborn to plot the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sb.barplot(data = grp_sizes, x = \"REGION\", y = 0, hue = \"FES\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we see that region 3 is the most populous and that the same basic patterns tend to continue across regions. Perhaps we notice that region 4 (the West) is unusual in that category 2 (\"2 .Married-couple family: Husband in labor force, wife not in LF\") is more common than category 4 (\"4 .Married-couple family: Neither husband nor wife in LF\"). This may indicate a generally younger population with fewer retired couples.\n",
        "\n",
        "We could be more precise if we had a table with each region as a row and each FES category as a column.\n",
        "\n",
        "To view these results as a table, it is easier to \"unstack\" the\n",
        "results, which moves one level of the row index to the columns.\n",
        "This is also called \"pivoting\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grp_size_tbl = grp.size().unstack()\n",
        "grp_size_tbl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can ask for which region is category 2 more common than 4?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grp_size_tbl[2.0] > grp_size_tbl[4.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the `unstack` method on the grouped medians. Save the result to a variable, but also display it. What are the rows and columns of the result?\n",
        "\n",
        "\n",
        "<details>\n",
        "    \n",
        "```\n",
        "grp_median = grp[\"HINCP\"].median() # you probably don't have to recreate, but just to be clear\n",
        "\n",
        "gms = grp_median.unstack()\n",
        "gms\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "In which categories does region 1 have higher median income than region 4? (Recall the use of `.loc` to select rows of a table.) Use a comparison to find out.\n",
        "\n",
        "\n",
        "<details>\n",
        "    \n",
        "```\n",
        "gms.loc[1.0] > gms.loc[4.0]\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "This analysis shows that it is possible to gain insight about two\n",
        "explanatory factors by stratifying the data on the _cross product_\n",
        "consisting of all subgroups defined by pairs of levels of the two\n",
        "factors.  These subgroups are sometimes called _cells_.  It should\n",
        "be clear however that this approach will not \"scale\" very well --\n",
        "cross products for three or more factors will generally produce huge\n",
        "numbers of cells.  We may have too little data per cell to produce\n",
        "meaningful estimates of population quantities, and even if this is\n",
        "not an issue, there will be a huge number of combinations to\n",
        "consider.  The difficulty of _controlling_ for, or stratifying on\n",
        "multiple explanatory factors is one of the major challenges that\n",
        "arises in statistical data analysis.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}